# 6 数据处理与拟合模型
[[0 Python数据分析]]
# 6.1 数据与大数据
### 6.1.1 数与数据
+ 我们把信息的不同形式称之为“**模态**”，包括：
	- 数值类数据，例如结构化的excel表格和SQL文件。
	- 文本类数据，例如新闻报道、微博评论、餐饮点评等文字。
	- 图像类数据，以一定尺寸的黑白或彩色图像在计算机内存储。
	- 音频类数据，例如音乐、电话录音等。
	- 信号类数据，例如地震波的波形、电磁波信号、脑电信号等。
+ 这些形式下的数据都可以用来数学建模。文本变为数据，只需要做词频统计、TF-IDF、词嵌入等操作就可以“数化”；图像变为数据，因为它在计算机内本身就是以RGB三个通道的大矩阵在存储每个像素点的颜色信息；音频和信号类数据则更为简单，它本身可以视作一个波，用信号与系统的概念和方法也可以对其“数化”。
+ 而不同模态的数据往往能够联合对同一个事物去做描述，例如对于狗狗而言，既有它的图片，又有它的叫声。视频本身也是由一帧帧图像在时间轴上堆叠才形成的数据。如果加上声音就更完美了。所以本质上视频数据也是图像和音频的融合。能够同时利用描述同一事物的不同模态数据去进行联合建模的模型，我们称之为**多模态**模型。

### 6.1.2 数据与大数据
+ 面对不同的数据，能够使用最合适的方法最为重要。而判断此方法是否合适，一个根本的衡量就是**数据的体量**。

### 6.1.3 数据科学的研究对象
+ Drew Conway：**数据科学是统计学、计算机科学和领域知识的交叉学科**![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image.png)
+ 大数据科学研究的不仅仅是数据分析，还包括：
	+ 数据的**获取和存储**：包括爬虫、软件定义存储、硬件存储有关背景知识等。
	- 数据的**处理**：包括分布式计算、并行计算、数据流等知识，以及Hadoop、Spark等大数据框架。
	- 数据的**分析**：包括统计学、数据挖掘与机器学习、计算机视觉、自然语言处理等内容，重在挖掘数据中的模式与知识。
	- 数据的**管理**：现代数据库系统及其架构等内容。
	- 数据的**应用**：数据可视化、数据相关软件的开发、报表分析以及如何将数据挖掘得到的结果还原为实际问题的解决方案。

## 6.2 数据的预处理
### 6.2.1 为什么需要数据预处理
+ 在现实生活问题中，我们得到的原始数据往往非常混乱、不全面，模型往往无法从中有效识别并提取信息。数据和特征决定了效果的上限，而模型和算法只是逼近这个上限而已，在采集完数据后，建模的首要步骤以及主要步骤便是数据预处理
+ 我们先来引入一些概念：![image-1.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-1.png)
+ 最上面一行我们称之为**表头**，表头的每一格我们称之为**字段**。每个字段描述了这一列数据表示的意义。而这个表格的**体量**则是它有多少行多少列，如果列数超过了行数的1/2就可以说是有些稀疏了，如果列数是行数的3倍那它就是严重稀疏的数据。这个数据的每一列称其为一个**属性**，有多少个属性又可以称之有多少维。
	+ 注意：**稀疏**还有一种定义，就是**表格里面很多都是空白的或者全是0**。
+ 属性有**离散属性**和**连续属性**之分。例如在图中，“品牌类型”这个属性只有{1,2,3}三个有限的取值，我们称之为离散。当然，这个取值也可以不是数字，比如{汽车，火车，飞机}等。但例如属性“a1”，它的取值是一个连续的浮点数，这种属性我们称之为连续属性。连续属性和离散属性的处理方法是截然不同的。“目标客户编号”提供了每一行数据的标识信息，可以根据这个编号对数据进行检索、排序等操作，我们称之为“主码”，也可以理解为id。
### 6.2.2 Pandas处理数据的基础
[[3 Python数据分析之Pandas]]

### 6.2.3 数据的规约
+ 数据为何需要规约？因为实际应用中数据的分布可能是有偏的，量纲影响和数值差异可能会比较大。规约是为了形成对数据的更高效表示，学习到更好的模型。它会保留数据的原始特征，但对极端值、异常值等会比较敏感。这里我们就介绍两个比较典型的规约方式：**min-max规约和Z-score规约**。（其实就是**标准化**）
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240722104054.png)

## 6.3 常见的统计分析模型
### 6.3.1 回归分析与分类分析
[[3 线性回归]]
[[4 逻辑回归]]

### 6.3.2 假设检验
+ 假设检验是统计分析中重要的一环，它贯穿着统计分析的所有环节。在数学建模中，我们既能通过假设检验对数据进行探索性的信息挖掘，以给予我们选择模型充分且客观的依据；又能在建模完成后，通过特定的假设检验验证模型的有效性。因此，储备基本的假设检验知识，学会根据数据特点、任务需求选择相对应的假设检验十分重要。

+ 根据样本信息与已知信息，对一个描述总体性质的命题进行“是或否”的检验与回答，就是假设检验的本质。即：假设检验验证的不是样本本身的性质，而是样本所在总体的性质。
+ 假设检验大体上可分为两种：**参数假设检验**与**非参数假设检验。** 若假设是关于总体的一个参数或者参数的集合，则该假设检验就是参数假设检验，刚刚的例子的假设是关于总体均值的，均值是参数，因此这是一个参数假设检验；若假设不能用一个参数集合来表示，则该假设检验就是非参数假设检验，一个典型就是正态性检验。
+ **正态性检验**：于参数检验比非参数检验更灵敏，因此一旦数据是正态分布的，我们应该使用参数检验，此时对数据进行正态性检验就非常有必要了。在这里，我们提供了三种方法对帮助大家判断数据的正态性：可视化判断-正态分布概率图；Shapiro-Wilk检验；D'Agostino's K-squared检验。
```python
# 生成1000个服从正态分布的数据
data_norm = stats.norm.rvs(loc=10, scale=10, size=1000) # rvs(loc,scale,size):生成服从指定分布的随机数，loc：期望；scale：标准差；size：数据个数
# 生成1000个服从卡方分布的数据
data_chi=stats.chi2.rvs(2,3,size=1000)

# 画出两个概率图
fig=plt.figure(figsize=(12,6))
ax1=fig.add_subplot(1,2,1)
plot1=stats.probplot(data_norm,plot=ax1) # 正态数据
ax2=fig.add_subplot(1,2,2)
plot2=stats.probplot(data_chi,plot=ax2) # 卡方分布数据
```
![image-4.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-4.png)
+ 可以看到，正态性数据在正态分布概率图中十分接近直线y=x，而卡方分布数据则几乎完全偏离了该直线。可见，概率图可视化确实可以起到初步判断数据正态性的作用。实际应用中，由于数据的复杂性，仅使用一种方法判断正态性有可能产生一定的误差，因此我们通常同时使用多种方法进行判断。如果不同方法得出的结论不同，此时就需要仔细观察数据的特征，寻找结果不一致的原因。如：若Shapiro-Wilk test显著（非正态），D'Agostino's K-squared test不显著（正态），则有可能是因为样本量较大，或者样本中存在重复值现象，如果事实确实如此，那么我们就应该采纳D'Agostino's K-squared test的结论而非Shapiro-Wilk test的结论。（在这里，我们假设数据服从正态分布，p<0.05则拒绝假设，换个说法：p<0.05数据不服从正态分布，如果p>=0.05说明没有证据说明数据不服从正态分布）
```python
# 接下来，我们在python中定义一个函数，将概率图、Shapiro-Wilk test、D'Agostino's K-squared test结合在一起。
data_small = stats.norm.rvs(0, 1, size=30) # 小样本正态性数据集
data_large = stats.norm.rvs(0, 1, size=6000) # 大样本正态性数据集
# 定义一个正态性检验函数，它可以输出：
## 正态概率图
## 小样本Shapiro-Wilk检验的p值
## 大样本D'Agostino's K-squared检验的p值

from statsmodels.stats.diagnostic import lilliefors
from typing import List

def check_normality(data: np.ndarray, show_flag: bool=True) -> List[float]:
    """
    输入参数
    ----------
    data : numpy数组或者pandas.Series
    show_flag : 是否显示概率图
    Returns
    -------
    两种检验的p值；概率图
    """

    if show_flag:
        _ = stats.probplot(data, plot=plt)
        plt.show()

    pVals = pd.Series(dtype='float64')
    # D'Agostino's K-squared test
    _, pVals['Omnibus'] = stats.normaltest(data) 

    # Shapiro-Wilk test
    _, pVals['Shapiro-Wilk'] = stats.shapiro(data)

    print(f'数据量为{len(data)}的数据集正态性假设检验的结果 : ----------------')
    print(pVals)
check_normality(data_small,show_flag=True)
```
![image-5.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-5.png)
```python
数据量为30的数据集正态性假设检验的结果 : ----------------
Omnibus         0.703004
Shapiro-Wilk    0.898077
dtype: float64

check_normality(data_large,show_flag=False) # 当样本量大于5000，会出现警告
数据量为6000的数据集正态性假设检验的结果 : ----------------
Omnibus         0.980014
Shapiro-Wilk    0.992427
dtype: float64

e:\anaconda\envs\ml\lib\site-packages\scipy\stats\morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000.
  warnings.warn("p-value may not be accurate for N > 5000.")
```

+ **单组样本均值假定的检验**：必胜中学里，陈老师班结束了一次英语考试。由于班级人数较多，短时间内很难完成批改与统计，陈老师又很想知道此次班级平均分与级长定的班级均分137的目标是否有显著区别，于是他随机抽取了已经改好的10名同学的英语成绩：
136,136,134,136,131,133,142,145,137,140
问：陈老师可以认为此次班级平均分与级长定的班级均分137的目标没有显著区别吗？

很明显，这就是一个典型的单组样本均值假定的检验，比较的是这个样本（10个同学的英语成绩）所代表的总体均值（班级英语成绩均值）是否与参考值137相等。那么对于这类问题，我们有两种检验可以使用：**单样本t检验与wilcoxon检验**。
```python
data=np.array([136,136,134,136,131,133,142,145,137,140])
data
```
?
### 6.3.3 随机过程与随机模拟
https://datawhalechina.github.io/intro-mathmodel/#/CH6/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E6%8B%9F%E5%90%88%E6%A8%A1%E5%9E%8B?id=_632-%e9%9a%8f%e6%9c%ba%e8%bf%87%e7%a8%8b%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a8%a1%e6%8b%9f

## 6.4 数据可视化
[[4 Python数据分析之Matplotlib]]
[[6 Python数据分析之Seaborn]]
[[7 Python数据分析之Plotnine]]

## 6.5 插值模型
+ 插值方法实际上除了是用于处理缺失值一个很好的方法，也是做数据填充的很好的方法。比如，我获取了按日的数据，需要按小时对数据进行填充，这个时候就可以用插值去做填充任务。这里介绍几种比较常见的插值方法。

### 6.5.1 线性插值法
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240722105244.png)
```python
import numpy as np
#数据准备
X = np.arange(-2*np.pi, 2*np.pi, 1) # 定义样本点X，从-pi到pi每次间隔1
Y = np.sin(X) # 定义样本点Y，形成sin函数
new_x = np.arange(-2*np.pi, 2*np.pi, 0.1) # 定义插值点
# 进行样条插值
import scipy.interpolate as spi
# 进行一阶样条插值
ipo1 = spi.splrep(X, Y, k=1)  # 样本点导入，生成参数
iy1 = spi.splev(new_x, ipo1)  # 根据观测点和样条参数，生成插值
```

### 6.4.2 三次样条插值
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240722105320.png)
```python
# 进行三次样条拟合
ipo3 = spi.splrep(X, Y, k=3)  # 样本点导入，生成参数
iy3 = spi.splev(new_x, ipo3)  # 根据观测点和样条参数，生成插值
```

### 6.4.3 拉格朗日插值
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240722105805.png)
```python
def lagrange(x0,y0,x):
    y=[]
    for k in range(len(x)):
        s=0
        for i in range(len(y0)):
           t=y0[i]
           for j in range(len(y0)):
              if i!=j:
                t*=(x[k]-x0[j])/(x0[i]-x0[j])
           s+=t
        y.append(s)
    return y
```
+ 通过一个简单的样例进行测试：
```python
from scipy.interpolate import interp1d
x0=[1,2,3,4,5]
y0=[1.6,1.8,3.2,5.9,6.8]
x=np.arange(1,5,1/30)
f1=interp1d(x0,y0,'linear')
y1=f1(x)
f2=interp1d(x0,y0,'cubic')
y2=f2(x)
y3=lagrange(x0,y0,x)
plt.plot(x0,y0,'r*')
plt.plot(x,y1,'b-',x,y2,'y-',x,y3,'r-')
plt.show()
```
![image-39.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-39.png)
